# Journal — 2025-09-20 — Day 2 - More SQL, Joins, & Data Modeling

## 1) What I learned (bullets, not prose)
- Models in dbt = representations of tables (from {{ source('raw', 'autompg___cars_layn') }})=
- Chinook Database: sample DB simulating a digital media store (Artists, Albums, Tracks, Customers, Invoices, Employees); "chinook" = warm wind
- Schema = organizational structure of a DB
- Data Modeling = defining entities, attributes, and relationships
- Business perspective: identify what info matters (e.g., sales data)

- Entity-Relationship Model:
    - Entities = tables (e.g., Customer, Order)
    - Attributes = properties (e.g., Customer Name, Order Date)
    - Relationships = rules (e.g., a customer can place many orders)

- SQL = bridge between business questions and data
    - Famous because it’s easy to understand & implement
    - Core clauses: FROM, WHERE, GROUP BY, HAVING, SELECT, ORDER BY, LIMIT

- Normalization (1NF, 2NF, 3NF/BCNF) = reduces anomalies; used in OLTP
- Transition: OLTP (normalized, transactions) → OLAP (denormalized, analytics/star schema)

- Data Modeling layers:
    - Conceptual = business entities
    - Logical = keys, relationships, constraints
    - Physical = DB-specific datatypes, indexes, partitions

- Dimensional modeling workflow (Kimball):
    1) Select process
    2) Declare grain
    3) Identify dimensions
    4) Identify facts
    5) Conform dimensions
    6) Load/test
    7) Query/visualize

- Facts = verbs/measures; Dimensions = nouns/descriptions
- Postgres as source system; 
- ClickHouse as analytics store

## 2) New vocabulary (define in your own words)
- **term** — my definition
- **SQL** — Structured Query Language; used to create, read, update, and delete data in relational databases.
- **Chinook** — sample database for learning; models a digital media store with customers, invoices, artists, albums, tracks, etc.
- **Schema** — blueprint or structure of how tables, relationships, and constraints are organized in a database.
- **Entity** — a real-world object represented as a table in a database (e.g., Customer, Order).
- **Attribute** — a property or field of an entity (e.g., Customer Name, Order Date).
- **Relationship** — logical connection between entities (e.g., one customer can have many invoices).
- **Normalization** — process of structuring a database to reduce redundancy and improve consistency (1NF, 2NF, 3NF/BCNF).
- **Star Schema** — dimensional modeling approach with a central fact table surrounded by dimension tables, optimized for analytics.
- **Fact Table** — table that stores measurable events/transactions (e.g., sales, revenue, quantities).
- **Dimension Table** — descriptive table with attributes that give context to facts (e.g., customer details, product info).
- **Grain** — the level of detail (granularity) captured in a fact table (e.g., sales per line item vs per invoice).

## 3) Data Engineering mindset applied (what principles did I use?)
- Normalize where needed for integrity; denormalize for analytics
- Think in layers: business → logical → physical models
- Practice building facts/dimensions for reporting clarity

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Started with normalization to understand anomalies → then shifted to dimensional modeling for analytics
- Used NULL handling and joins in Chinook queries — assumption: better clarity than sentinel values
- Trade-off: normalized schemas are efficient for OLTP but not analytics; star schema introduces redundancy but speeds up queries

## 5) Open questions (things I still don’t get)
- Where should I put my dimensional and fact table in the clean or mart?

## 6) Next actions (small, doable steps)
- [ ] study intermediate dlt
- [ ] study intermediate dbt
- [ ] practice more joins and aggregations in SQL
- [ ] compare star schema vs snowflake schema in practic
- [ ] study how to code in python of pipeline

## 7) Artifacts & links (code, queries, dashboards)
- https://dataengineering.ph/

*--left join allows rows from left table, even if no match;* 
*cuctomers with or w/o invoices*
*help answer:  who hasnt bought anything yet?*

select customer.first_name , invoice.invoice_id
from customer
left join invoice
on customer.customer_id = invoice.customer_id ;

**top artist by revenue**
SELECT ar.name AS artist,
       SUM(il.unit_price * il.quantity) AS total_revenue
FROM invoice_line il
JOIN track t ON il.track_id = t.track_id
JOIN album al ON t.album_id = al.album_id
JOIN artist ar ON al.artist_id = ar.artist_id
GROUP BY ar.artist_id, ar.name
ORDER BY total_revenue DESC
LIMIT 1;
*output: artist iron maiden total_revenue = 138.6*

**-- Dimension: Customer**
CREATE TABLE sandbox.DimCustomer_layn
ENGINE = MergeTree
ORDER BY tuple() AS
SELECT
    customer_id AS customer_key,
    first_name,
    last_name,
    country
FROM raw.chinook___customer_layn;

**-- Fact: Invoice Line**
CREATE TABLE sandbox.FactInvoiceLine_layn
ENGINE = MergeTree
ORDER BY tuple() AS
SELECT
    il.invoice_line_id AS invoice_line_key,
    c.customer_id AS customer_key,
    t.track_id AS track_key,
    i.invoice_date,
    il.quantity,
    il.unit_price * il.quantity AS line_amount
FROM raw.chinook___invoice_line_layn il
JOIN raw.chinook___invoice_layn i  ON il.invoice_id = i.invoice_id
JOIN raw.chinook___customer_layn c ON i.customer_id = c.customer_id
JOIN raw.chinook___track_layn t    ON il.track_id = t.track_id;

### Mini reflection (3–5 sentences)
Today deepened my understanding of normalization vs dimensional modeling. I realized how OLTP schemas prioritize integrity, while OLAP schemas prioritize query performance. The Chinook database gave me a business-like context to practice joins and aggregations. Next time, I’ll pay closer attention to spotting anomalies and deciding when to switch from normalized to star schema.

### BONUS: What is a meme that best describes what you feel or your learning today?

![Alt text](../assets/day2-meme.png "data modelling...?")