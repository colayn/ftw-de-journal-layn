# ğŸ—“ï¸ Journal â€” 10-11-2025 â€” DAY 5 - Data Collection & Web Scraping 101

## 1) What I learned (bullets, not prose)
- Learned how data flows through its lifecycle â€” from **source to serving** and finally to **decision-making**.  
- Understood **different data formats** (CSV, JSON, XML, Parquet) and why we should pick formats based on use case, not convenience.  
- Discovered how **pandas** and **ibis** help with data analysis and system connections.  
- Practiced **API handling** â€” learning about endpoints, parameters, authentication, and rate limits.  
- Learned how **web scraping** works, including identifying static vs dynamic pages and how to handle JavaScript-rendered data.  
- Realized how important **data ethics** and **data quality checks** are â€” we should always collect data responsibly and maintain accuracy.  
- Understood that **data engineering isnâ€™t just about code** â€” itâ€™s also about being ethical, organized, and thoughtful in how we collect and use data.

---

## 2) New vocabulary (define in your own words)
- **venv** - isolate modules; library
- **Primitive Data Type** â€” Basic data types like integer, float, boolean, and string.  
- **Temporal Data** â€” Data with time-related values like date or timestamp.  
- **Categorical Data** â€” Data classified into categories like labels or enums.  
- **API (Application Programming Interface)** â€” A way for systems to talk to each other, usually using HTTP methods (GET, POST, etc.).  
- **Endpoint** â€” The specific URL where an API request is sent.   
- **Web Scraping** â€” Extracting information from websites using tools like BeautifulSoup or Playwright.  
- **Ibis** â€” A Python library that connects to multiple data systems and turns queries into DataFrames easily.  
- **Data Ethics** â€” Making sure that how we collect, store, and use data is transparent, fair, and responsible.  
- **Parquet/ORC** â€” A compressed, columnar file format efficient for large datasets.
- **BeautifulSoup** â€” A Python library for parsing and scraping HTML.
- **Headless browser** â€” A browser automation tool (like Playwright) that runs without a visible window.

---

## 3) Data Engineering mindset applied (what principles did I use?)
- **Validate everything** â€” Row counts, schemas, timestamps before loading data.  
- **Pick the right tool for the job** â€” Pandas for quick work, ibis for large systems.  
- **Think ethically** â€” Just because we *can* collect data doesnâ€™t mean we *should*.  
- **Maintain data quality** â€” Clean, consistent, and verified data is the foundation for trust.

---

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Used **pandas** for smaller workflows because itâ€™s simple and beginner-friendly.  
- Used **ibis** for connecting to multiple backends â€” scalable and easier to manage in larger systems.  
- Chose **JSON** format when working with APIs since itâ€™s flexible and commonly used.  
- Decided to extract only small slices of data (using LIMIT or filters) to keep it manageable and efficient.  
- Assumed that **data quality checks** (like null detection, schema validation, and type consistency) are part of every collection process.

---

## 5) Open questions (things I still donâ€™t get)
- How do we automate **data quality checks** in real-world pipelines?  
- How can I handle APIs with strict rate limits efficiently?
- How can bias in data collection be reduced technically (not just ethically)?  
- Whatâ€™s the best way to log and monitor **API calls** efficiently?

---

## 6) Next actions (small, doable steps)
- [ ] Try to do the remaining activities on Sir Myk's git repo.
- [ ] Study **Git** deeper for version control.  
- [ ] Try using **ibis** with DuckDB and PostgreSQL connections.  
- [ ] Practice scraping simple static sites with **BeautifulSoup**.  
---

## 7) Artifacts & links (code, queries, dashboards)
- [Data Engineering PH](https://dataengineering.ph/)  
- [Pandas Documentation](https://pandas.pydata.org/)  
- [Ibis Project](https://ibis-project.org/)  
- [Open Meteo API](https://open-meteo.com/en/docs)  
- [Quotes API (Practice)](https://quotes.toscrape.com/api/quotes)

---

## NOTE: Data Quality Checks
To maintain accuracy and trust in collected data:
- âœ… Validate schema â€” check if column names and data types match expectations.  
- âœ… Check for missing or null values â€” avoid incomplete records.  
- âœ… Ensure unique IDs â€” prevent duplicates.  
- âœ… Verify data freshness â€” use timestamps (e.g., updated_at).  
- âœ… Compare row counts â€” before and after transformations.  
- âœ… Track lineage â€” always know where data came from.  
- âœ… Log errors and data anomalies for review.

> â€œGood data quality = good decisions.â€

---

### Mini reflection (3â€“5 sentences)
Today I learned that most data comes from APIs and websites, and collecting it responsibly is essential. Web scraping was fun but challenging, especially with dynamic pages. I realized the importance of handling API rate limits, validating data quality, and being mindful of ethics. Moving forward, Iâ€™ll focus on ensuring clean, unbiased, and reliable data collection in every project.

---

### BONUS: Meme that fits my learning today
![Alt text](../assets/day5-meme.jpg "what is a data engineer?")
