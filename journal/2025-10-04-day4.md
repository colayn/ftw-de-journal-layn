# Journal — 2025-10-04 — Day 4 - Data Quality, Review of DE Workflow

## 1) What I learned (bullets, not prose)
- Importance of documentation
- Solutioning first vs problem framing — learn to formulate business questions before solutions
- Trade-offs in Snowflake (speed vs understandability, avoid too many views)
- Data modeling changes over time (example: procurement system)
- Pull from past experiences and define core vs sub-questions
- Know your audience — operational vs strategic
- Always validate data with experts and business rules
- Create infrastructure/legacy systems that persist over time
- Business questions: make money, save money, ensure compliance
- DE builds robust, well-documented systems that people trust
- You’ll never know everything — stay curious

## 2) New vocabulary (define in your own words)
- **MTTD / MTTR** — Mean Time to Detect / Resolve, used to measure incident response speed
- **Lineage** — Traceability of data from source to destination
- **KYC** — Know Your Customer, verifying client identity and behavior
- **SLA** — Service Level Agreement, defines acceptable performance thresholds

## 3) Data Engineering mindset applied (what principles did I use?)
- Keep raw data intact; clean in stages
- Build trust with accuracy, transparency, and timeliness
- Layered data quality checks across ingestion, clean, and mart
- Document processes to ensure legacy and reproducibility

## 4) Decisions & assumptions (why, alternatives, trade-offs)
- Used layered DQ checks → better traceability but adds complexity
- Logged DQ trends → more storage, but helps monitoring
- Used dbt over manual SQL → less flexibility but stronger documentation
- Preferred null validation over dropping rows → preserves completeness

## 5) Open questions (things I still don’t get)
- Best practices for filtering low-quality data
- When to enforce DQ thresholds (warn vs fail)
- How to automate DQ alerts in production

## 6) Next actions (small, doable steps)
- [ ] Review dbt test configurations
- [ ] Explore DQ dashboard metrics (% passed, freshness, nulls)
- [ ] Practice SQL checks on ClickHouse
- [ ] Research lineage tracking tools

## 7) Artifacts & links (code, queries, dashboards)
- https://dataengineering.ph/

---

### Mini reflection (3–5 sentences)
Today’s session showed how data quality defines trust in analytics. I learned that clean data is not enough — documentation, lineage, and ownership matter equally. I’ll be more mindful of thresholds and when to log vs alert. The DQ layers (raw → clean → mart) made it clear where checks should live. Next time, I’ll apply dbt tests earlier in the workflow.

### BONUS: What is a meme that best describes what you feel or your learning today?
![Alt text](../assets/meme.png "what is a data engineer?")

